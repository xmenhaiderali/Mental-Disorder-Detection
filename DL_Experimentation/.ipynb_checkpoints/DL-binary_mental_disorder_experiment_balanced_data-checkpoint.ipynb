{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e21a07-9d57-42e9-a2d9-cea2cbd7342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ali\n",
      "[nltk_data]     Haider\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ali\n",
      "[nltk_data]     Haider\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import advertools\n",
    "import re\n",
    "import spacy\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report,auc, \n",
    "                            roc_auc_score, precision_score,\n",
    "                            recall_score,f1_score, accuracy_score)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import nltk\n",
    "import nltk.util\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, SpatialDropout1D, LSTM, Conv1D, MaxPool1D, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ee10c3-bad3-4bf9-bffc-128789be6257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6928cf4-ec7c-4097-9b8e-ed76aaf813e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent/\"Data\"\n",
    "final_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1da8ed65-7e05-4572-9ed2-8e107029f08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>\"@USER That feelingtake good care ðŸ’—xx\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>\"@USER This had me belly laughing ðŸ˜‚\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>\"Solid tactics from Simeone.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>3. \"Partey's control in the middle is unmatched.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PTSD</td>\n",
       "      <td>\"Relationships where both people have conceale...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class                                              tweet\n",
       "0  SCHIZOPHRENIA             \"@USER That feelingtake good care ðŸ’—xx\"\n",
       "1           ADHD               \"@USER This had me belly laughing ðŸ˜‚\"\n",
       "2        CONTROL                      \"Solid tactics from Simeone.\"\n",
       "3        CONTROL  3. \"Partey's control in the middle is unmatched.\"\n",
       "4           PTSD  \"Relationships where both people have conceale..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.read_csv(\"control_data.csv\")\n",
    "final_data.rename(columns={\"Disorder\":\"class\"},inplace=True)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d00ea73f-d82f-44de-b680-321f9a781268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "CONTROL            18761\n",
       "ADHD                3034\n",
       "SCHIZOPHRENIA       2970\n",
       "OCD                 2905\n",
       "ANXIETY             2729\n",
       "PTSD                2466\n",
       "DEPRESSION          2161\n",
       "AUTISM              1425\n",
       "EATING DISORDER      403\n",
       "BIPOLAR              244\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4806db0c-48fb-4932-8d2a-05c954986b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = final_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c87c5d-119b-4790-8344-1b9d83a534fe",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4ae8d3b-8902-4979-a434-6341ea4258fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>\"@USER That feelingtake good care ðŸ’—xx\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class                                   tweet\n",
       "0  SCHIZOPHRENIA  \"@USER That feelingtake good care ðŸ’—xx\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "724da0c0-dab7-41cd-baaf-e8bc3382621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str) -> str:\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)\n",
    "    emoji_token = advertools.extract_emoji(tokens[0])\n",
    "    emoji_token = emoji_token[\"emoji_text\"]\n",
    "    for tok in range(0,len(emoji_token)):\n",
    "        if emoji_token[tok]:\n",
    "            tokens[0][tok] = \" \".join(emoji_token[tok])\n",
    "    clean_text = \" \".join(tokens[0])\n",
    "    clean_pattern = r\"@\\w+|#\\w+|\\W+|x+|https\\.\\//www\\.(\\w+|\\W+)\\.com|http\\w*|www\\.(\\w+|\\W+)\\.com|user\\w*|\\d+\"\n",
    "    clean_text = re.sub(clean_pattern,\" \",clean_text).strip()\n",
    "    clean_text = re.sub(r\"\\s+\",\" \",clean_text).strip()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfd1d22c-815a-4938-8685-0afc1297ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatize(text: str) -> str:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)\n",
    "    text = [lemma.lemmatize(tok) for tok in tokens[0]]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fd6338c-b672-45ed-831a-471f8a3edb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spell_check(text:str) -> str:\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     nlp.add_pipe(\"contextual spellchecker\")\n",
    "#     doc = nlp(text)\n",
    "#     return doc._.outcome_spellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dbfa883-befb-403f-862f-4c8caf70af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text: str) -> str:\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)[0]\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    clean_word = [tok for tok in tokens if tok not in stop_words]\n",
    "    return \" \".join(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9636e634-0a99-4660-b38e-d5b3d94f6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_grams(text: str, n: list[int]) -> list[tuple]:\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)[0]\n",
    "    if len(n)<=1:\n",
    "        n_grams = list(nltk.ngrams(tokens,n[0],pad_right=True,right_pad_symbol=\"</s>\"))\n",
    "        bag_ngrams = \" \".join([\"_\".join(c) for c in n_grams])\n",
    "    else:\n",
    "        bag_ngrams = \" \"\n",
    "        for n_gram in range(n[0],n[1]+1):\n",
    "            n_grams = list(nltk.ngrams(tokens,n_gram,pad_right=True,right_pad_symbol=\"</s>\"))\n",
    "            if n_gram==1:       \n",
    "                n_grams = \" \".join([\"\".join(c[0]) for c in n_grams])\n",
    "                bag_ngrams+=n_grams\n",
    "            else:\n",
    "                n_grams = \" \".join([\"_\".join(c) for c in n_grams])\n",
    "                bag_ngrams+=\" \"+n_grams\n",
    "    return bag_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30a93b6f-c571-47c6-a5c2-8545bf20044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(train: pd.DataFrame, test:pd.DataFrame()):\n",
    "    tf_idf = TfidfVectorizer()\n",
    "    train_feat = tf_idf.fit_transform(train)\n",
    "    test_feat = tf_idf.transform(test)\n",
    "    return train_feat,test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37b5564a-396e-4e58-ab02-2f6238815dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classification_metrics(actuals: np.array, preds: np.array) -> pd.DataFrame:\n",
    "    metrics_performance = pd.DataFrame()\n",
    "    precision_0 = precision_score(actuals,preds, pos_label=0)\n",
    "    precision_1 = precision_score(actuals,preds, pos_label=1)\n",
    "    recall_0 = recall_score(actuals,preds,pos_label=0)\n",
    "    recall_1 = recall_score(actuals,preds,pos_label=1)\n",
    "    f1_0 = f1_score(actuals,preds,pos_label=0)\n",
    "    f1_1 = f1_score(actuals,preds,pos_label=1)\n",
    "    accuracy = accuracy_score(actuals,preds)\n",
    "    metrics_performance[\"precision_0\"] = [precision_0]\n",
    "    metrics_performance[\"precision_1\"] = [precision_1]\n",
    "    metrics_performance[\"recall_0\"] = [recall_0]\n",
    "    metrics_performance[\"recall_1\"] = [recall_1]\n",
    "    metrics_performance[\"f1_0\"] = [f1_0]\n",
    "    metrics_performance[\"f1_1\"] = [f1_1]\n",
    "    metrics_performance[\"accuracy\"] = [accuracy]\n",
    "    return metrics_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b92ae273-09ce-457a-8b15-b2241bb0ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_pipeline(data_preprocess: pd.DataFrame,\n",
    "                             # ngram_range: list,\n",
    "                            filename: str,\n",
    "                            ) -> pd.DataFrame:\n",
    "    data_preprocess = data_preprocess[~(data_preprocess[\"tweet\"].isnull())]\n",
    "    data_preprocess = data_preprocess[[\"class\",\"tweet\"]]\n",
    "    data_preprocess.rename(columns={\"class\":\"Disorder\"}, inplace=True)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(clean_text)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(text_lemmatize)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(remove_stopword)\n",
    "    data_preprocess = data_preprocess[data_preprocess[\"tweet\"]!=\"\"]\n",
    "    # data_preprocess[\"text_features\"] = data_preprocess[\"tweet\"].apply(lambda text: \n",
    "    #                                                                   generate_n_grams(text,n=ngram_range))\n",
    "    print(\"saving preprocessed data\")\n",
    "    data_preprocess.to_csv(filename,index=False)\n",
    "    print(\"data saved\")\n",
    "    return data_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a50b3-be09-4e58-b50c-a4d38e450fd6",
   "metadata": {},
   "source": [
    "# balaned Dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83bb0659-74ce-47b5-ae20-4c4ecc4eeaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>\"@USER That feelingtake good care ðŸ’—xx\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class                                   tweet\n",
       "0  SCHIZOPHRENIA  \"@USER That feelingtake good care ðŸ’—xx\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e224662b-dce2-40a6-b846-a20f7c82a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessed data\n",
      "data saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disorder</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>feelingtake good care growing heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>belly laughing face tear joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>solid tactic simeone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>partey control middle unmatched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PTSD</td>\n",
       "      <td>relationship people concealed motif aware othe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Disorder                                              tweet\n",
       "0  SCHIZOPHRENIA                feelingtake good care growing heart\n",
       "1           ADHD                       belly laughing face tear joy\n",
       "2        CONTROL                               solid tactic simeone\n",
       "3        CONTROL                    partey control middle unmatched\n",
       "4           PTSD  relationship people concealed motif aware othe..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram_range = [1,3]\n",
    "data_preprocess = data_preprocess_pipeline(pipeline_data,\n",
    "                                           # ngram_range=ngram_range,\n",
    "                                           filename=\"dl_balanced_data_preprocess.csv\",\n",
    "                                           )\n",
    "data_preprocess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87d5adf0-bc2a-41a4-a220-f36e55d08f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disorder\n",
       "CONTROL            18640\n",
       "ADHD                3018\n",
       "SCHIZOPHRENIA       2959\n",
       "OCD                 2888\n",
       "ANXIETY             2719\n",
       "PTSD                2458\n",
       "DEPRESSION          2143\n",
       "AUTISM              1402\n",
       "EATING DISORDER      402\n",
       "BIPOLAR              243\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocess.Disorder.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7262134c-24b1-4586-87ed-1f6790fa62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_preprocess[\"tweet\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ba125f4-55bb-4e4f-b10d-2800c28fd5ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 12:24:45,589 | INFO | word2vec.py:582 | scan_vocab | collecting all words and their counts\n",
      "2024-08-18 12:24:45,637 | INFO | word2vec.py:565 | _scan_vocab | PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-08-18 12:24:45,798 | INFO | word2vec.py:565 | _scan_vocab | PROGRESS: at sentence #10000, processed 71168 words, keeping 10812 word types\n",
      "2024-08-18 12:24:45,888 | INFO | word2vec.py:565 | _scan_vocab | PROGRESS: at sentence #20000, processed 140479 words, keeping 14763 word types\n",
      "2024-08-18 12:24:45,904 | INFO | word2vec.py:565 | _scan_vocab | PROGRESS: at sentence #30000, processed 211155 words, keeping 17670 word types\n",
      "2024-08-18 12:24:45,962 | INFO | word2vec.py:588 | scan_vocab | collected 19345 word types from a corpus of 260267 raw words and 36872 sentences\n",
      "2024-08-18 12:24:45,969 | INFO | word2vec.py:637 | prepare_vocab | Creating a fresh vocabulary\n",
      "2024-08-18 12:24:46,384 | INFO | utils.py:447 | add_lifecycle_event | Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 19345 unique words (100.00% of original 19345, drops 0)', 'datetime': '2024-08-18T12:24:46.384582', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-08-18 12:24:46,386 | INFO | utils.py:447 | add_lifecycle_event | Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 260267 word corpus (100.00% of original 260267, drops 0)', 'datetime': '2024-08-18T12:24:46.386602', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-08-18 12:24:46,969 | INFO | word2vec.py:745 | prepare_vocab | deleting the raw counts dictionary of 19345 items\n",
      "2024-08-18 12:24:46,974 | INFO | word2vec.py:748 | prepare_vocab | sample=0.001 downsamples 25 most-common words\n",
      "2024-08-18 12:24:46,990 | INFO | utils.py:447 | add_lifecycle_event | Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 250369.46167272655 word corpus (96.2%% of prior 260267)', 'datetime': '2024-08-18T12:24:46.990895', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-08-18 12:24:47,265 | INFO | word2vec.py:805 | estimate_memory | estimated required memory for 19345 words and 300 dimensions: 56100500 bytes\n",
      "2024-08-18 12:24:47,265 | INFO | word2vec.py:863 | init_weights | resetting layer weights\n",
      "2024-08-18 12:24:47,379 | INFO | utils.py:447 | add_lifecycle_event | Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-08-18T12:24:47.379504', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-08-18 12:24:47,379 | INFO | utils.py:447 | add_lifecycle_event | Word2Vec lifecycle event {'msg': 'training model with 3 workers on 19345 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-08-18T12:24:47.379504', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-08-18 12:24:48,543 | INFO | word2vec.py:1608 | _log_progress | EPOCH 0 - PROGRESS: at 80.92% examples, 187415 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:24:48,631 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 0: training on 260267 raw words (250259 effective words) took 1.2s, 215170 effective words/s\n",
      "2024-08-18 12:24:49,684 | INFO | word2vec.py:1608 | _log_progress | EPOCH 1 - PROGRESS: at 96.20% examples, 233862 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:24:49,692 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 1: training on 260267 raw words (250423 effective words) took 1.0s, 241682 effective words/s\n",
      "2024-08-18 12:24:50,845 | INFO | word2vec.py:1608 | _log_progress | EPOCH 2 - PROGRESS: at 96.03% examples, 227823 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:24:50,861 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 2: training on 260267 raw words (250459 effective words) took 1.1s, 234372 effective words/s\n",
      "2024-08-18 12:24:51,710 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 3: training on 260267 raw words (250266 effective words) took 0.8s, 304635 effective words/s\n",
      "2024-08-18 12:24:52,777 | INFO | word2vec.py:1608 | _log_progress | EPOCH 4 - PROGRESS: at 84.44% examples, 202210 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:24:52,948 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 4: training on 260267 raw words (250505 effective words) took 1.2s, 204676 effective words/s\n",
      "2024-08-18 12:24:53,817 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 5: training on 260267 raw words (250307 effective words) took 0.7s, 334279 effective words/s\n",
      "2024-08-18 12:24:54,914 | INFO | word2vec.py:1608 | _log_progress | EPOCH 6 - PROGRESS: at 76.79% examples, 181908 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:24:55,105 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 6: training on 260267 raw words (250441 effective words) took 1.2s, 201928 effective words/s\n",
      "2024-08-18 12:24:56,224 | INFO | word2vec.py:1608 | _log_progress | EPOCH 7 - PROGRESS: at 61.79% examples, 153113 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:24:56,554 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 7: training on 260267 raw words (250407 effective words) took 1.3s, 188299 effective words/s\n",
      "2024-08-18 12:24:57,596 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 8: training on 260267 raw words (250335 effective words) took 0.9s, 275499 effective words/s\n",
      "2024-08-18 12:24:58,811 | INFO | word2vec.py:1608 | _log_progress | EPOCH 9 - PROGRESS: at 34.46% examples, 74034 words/s, in_qsize 6, out_qsize 3\n",
      "2024-08-18 12:24:59,634 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 9: training on 260267 raw words (250488 effective words) took 2.0s, 126090 effective words/s\n",
      "2024-08-18 12:25:00,676 | INFO | word2vec.py:1608 | _log_progress | EPOCH 10 - PROGRESS: at 80.81% examples, 196281 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:00,826 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 10: training on 260267 raw words (250393 effective words) took 1.2s, 212307 effective words/s\n",
      "2024-08-18 12:25:01,930 | INFO | word2vec.py:1608 | _log_progress | EPOCH 11 - PROGRESS: at 92.24% examples, 230431 words/s, in_qsize 2, out_qsize 1\n",
      "2024-08-18 12:25:01,960 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 11: training on 260267 raw words (250363 effective words) took 1.0s, 243870 effective words/s\n",
      "2024-08-18 12:25:02,861 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 12: training on 260267 raw words (250350 effective words) took 0.8s, 305012 effective words/s\n",
      "2024-08-18 12:25:03,938 | INFO | word2vec.py:1608 | _log_progress | EPOCH 13 - PROGRESS: at 69.44% examples, 164924 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:25:04,259 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 13: training on 260267 raw words (250351 effective words) took 1.4s, 182689 effective words/s\n",
      "2024-08-18 12:25:05,101 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 14: training on 260267 raw words (250328 effective words) took 0.8s, 302816 effective words/s\n",
      "2024-08-18 12:25:06,219 | INFO | word2vec.py:1608 | _log_progress | EPOCH 15 - PROGRESS: at 80.81% examples, 200099 words/s, in_qsize 5, out_qsize 1\n",
      "2024-08-18 12:25:06,668 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 15: training on 260267 raw words (250293 effective words) took 1.5s, 172495 effective words/s\n",
      "2024-08-18 12:25:07,731 | INFO | word2vec.py:1608 | _log_progress | EPOCH 16 - PROGRESS: at 84.44% examples, 205682 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:07,794 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 16: training on 260267 raw words (250331 effective words) took 1.1s, 229550 effective words/s\n",
      "2024-08-18 12:25:08,949 | INFO | word2vec.py:1608 | _log_progress | EPOCH 17 - PROGRESS: at 84.52% examples, 202214 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:09,093 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 17: training on 260267 raw words (250339 effective words) took 1.2s, 210604 effective words/s\n",
      "2024-08-18 12:25:10,323 | INFO | word2vec.py:1608 | _log_progress | EPOCH 18 - PROGRESS: at 88.23% examples, 202986 words/s, in_qsize 4, out_qsize 0\n",
      "2024-08-18 12:25:10,395 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 18: training on 260267 raw words (250239 effective words) took 1.2s, 216993 effective words/s\n",
      "2024-08-18 12:25:11,590 | INFO | word2vec.py:1608 | _log_progress | EPOCH 19 - PROGRESS: at 96.20% examples, 240319 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:25:11,606 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 19: training on 260267 raw words (250447 effective words) took 1.0s, 245299 effective words/s\n",
      "2024-08-18 12:25:12,522 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 20: training on 260267 raw words (250453 effective words) took 0.9s, 277723 effective words/s\n",
      "2024-08-18 12:25:13,757 | INFO | word2vec.py:1608 | _log_progress | EPOCH 21 - PROGRESS: at 80.55% examples, 193397 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:13,931 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 21: training on 260267 raw words (250579 effective words) took 1.2s, 205832 effective words/s\n",
      "2024-08-18 12:25:14,727 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 22: training on 260267 raw words (250543 effective words) took 0.7s, 340673 effective words/s\n",
      "2024-08-18 12:25:15,465 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 23: training on 260267 raw words (250494 effective words) took 0.7s, 347316 effective words/s\n",
      "2024-08-18 12:25:16,805 | INFO | word2vec.py:1608 | _log_progress | EPOCH 24 - PROGRESS: at 73.36% examples, 179067 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:25:17,068 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 24: training on 260267 raw words (250310 effective words) took 1.3s, 194451 effective words/s\n",
      "2024-08-18 12:25:18,045 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 25: training on 260267 raw words (250492 effective words) took 0.9s, 265556 effective words/s\n",
      "2024-08-18 12:25:18,912 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 26: training on 260267 raw words (250244 effective words) took 0.8s, 295654 effective words/s\n",
      "2024-08-18 12:25:19,967 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 27: training on 260267 raw words (250391 effective words) took 1.0s, 251483 effective words/s\n",
      "2024-08-18 12:25:21,051 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 28: training on 260267 raw words (250347 effective words) took 1.0s, 250370 effective words/s\n",
      "2024-08-18 12:25:21,973 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 29: training on 260267 raw words (250401 effective words) took 0.8s, 324542 effective words/s\n",
      "2024-08-18 12:25:23,010 | INFO | word2vec.py:1608 | _log_progress | EPOCH 30 - PROGRESS: at 73.17% examples, 177431 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:23,559 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 30: training on 260267 raw words (250290 effective words) took 1.6s, 159306 effective words/s\n",
      "2024-08-18 12:25:24,542 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 31: training on 260267 raw words (250292 effective words) took 1.0s, 258948 effective words/s\n",
      "2024-08-18 12:25:25,361 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 32: training on 260267 raw words (250385 effective words) took 0.8s, 310750 effective words/s\n",
      "2024-08-18 12:25:26,625 | INFO | word2vec.py:1608 | _log_progress | EPOCH 33 - PROGRESS: at 57.87% examples, 116824 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:25:26,968 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 33: training on 260267 raw words (250337 effective words) took 1.6s, 158190 effective words/s\n",
      "2024-08-18 12:25:28,124 | INFO | word2vec.py:1608 | _log_progress | EPOCH 34 - PROGRESS: at 92.08% examples, 230239 words/s, in_qsize 3, out_qsize 0\n",
      "2024-08-18 12:25:28,235 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 34: training on 260267 raw words (250175 effective words) took 1.1s, 225550 effective words/s\n",
      "2024-08-18 12:25:29,410 | INFO | word2vec.py:1608 | _log_progress | EPOCH 35 - PROGRESS: at 38.37% examples, 82963 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:25:30,060 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 35: training on 260267 raw words (250434 effective words) took 1.8s, 137890 effective words/s\n",
      "2024-08-18 12:25:31,272 | INFO | word2vec.py:1608 | _log_progress | EPOCH 36 - PROGRESS: at 100.00% examples, 242801 words/s, in_qsize 0, out_qsize 1\n",
      "2024-08-18 12:25:31,272 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 36: training on 260267 raw words (250387 effective words) took 1.0s, 241916 effective words/s\n",
      "2024-08-18 12:25:32,355 | INFO | word2vec.py:1608 | _log_progress | EPOCH 37 - PROGRESS: at 54.02% examples, 128335 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:32,804 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 37: training on 260267 raw words (250460 effective words) took 1.5s, 167542 effective words/s\n",
      "2024-08-18 12:25:33,864 | INFO | word2vec.py:1608 | _log_progress | EPOCH 38 - PROGRESS: at 96.03% examples, 237737 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:25:33,907 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 38: training on 260267 raw words (250452 effective words) took 1.1s, 237859 effective words/s\n",
      "2024-08-18 12:25:35,429 | INFO | word2vec.py:1608 | _log_progress | EPOCH 39 - PROGRESS: at 88.23% examples, 157829 words/s, in_qsize 4, out_qsize 0\n",
      "2024-08-18 12:25:35,475 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 39: training on 260267 raw words (250375 effective words) took 1.4s, 172905 effective words/s\n",
      "2024-08-18 12:25:36,389 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 40: training on 260267 raw words (250430 effective words) took 0.9s, 284464 effective words/s\n",
      "2024-08-18 12:25:37,323 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 41: training on 260267 raw words (250339 effective words) took 0.9s, 276477 effective words/s\n",
      "2024-08-18 12:25:38,524 | INFO | word2vec.py:1608 | _log_progress | EPOCH 42 - PROGRESS: at 84.65% examples, 203144 words/s, in_qsize 4, out_qsize 1\n",
      "2024-08-18 12:25:38,784 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 42: training on 260267 raw words (250402 effective words) took 1.3s, 192432 effective words/s\n",
      "2024-08-18 12:25:39,801 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 43: training on 260267 raw words (250342 effective words) took 0.9s, 278132 effective words/s\n",
      "2024-08-18 12:25:40,692 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 44: training on 260267 raw words (250419 effective words) took 0.9s, 285022 effective words/s\n",
      "2024-08-18 12:25:41,816 | INFO | word2vec.py:1608 | _log_progress | EPOCH 45 - PROGRESS: at 92.24% examples, 230812 words/s, in_qsize 2, out_qsize 1\n",
      "2024-08-18 12:25:41,841 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 45: training on 260267 raw words (250430 effective words) took 1.0s, 242849 effective words/s\n",
      "2024-08-18 12:25:42,948 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 46: training on 260267 raw words (250299 effective words) took 1.0s, 256381 effective words/s\n",
      "2024-08-18 12:25:43,991 | INFO | word2vec.py:1608 | _log_progress | EPOCH 47 - PROGRESS: at 77.08% examples, 189905 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:44,313 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 47: training on 260267 raw words (250410 effective words) took 1.3s, 188046 effective words/s\n",
      "2024-08-18 12:25:45,043 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 48: training on 260267 raw words (250341 effective words) took 0.7s, 366294 effective words/s\n",
      "2024-08-18 12:25:46,295 | INFO | word2vec.py:1608 | _log_progress | EPOCH 49 - PROGRESS: at 88.28% examples, 212345 words/s, in_qsize 4, out_qsize 0\n",
      "2024-08-18 12:25:46,401 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 49: training on 260267 raw words (250428 effective words) took 1.1s, 218366 effective words/s\n",
      "2024-08-18 12:25:47,501 | INFO | word2vec.py:1608 | _log_progress | EPOCH 50 - PROGRESS: at 84.55% examples, 202652 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:47,622 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 50: training on 260267 raw words (250340 effective words) took 1.2s, 214998 effective words/s\n",
      "2024-08-18 12:25:48,700 | INFO | word2vec.py:1608 | _log_progress | EPOCH 51 - PROGRESS: at 92.23% examples, 229597 words/s, in_qsize 3, out_qsize 0\n",
      "2024-08-18 12:25:48,840 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 51: training on 260267 raw words (250504 effective words) took 1.1s, 217973 effective words/s\n",
      "2024-08-18 12:25:50,001 | INFO | word2vec.py:1608 | _log_progress | EPOCH 52 - PROGRESS: at 34.57% examples, 82991 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:25:50,832 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 52: training on 260267 raw words (250336 effective words) took 1.9s, 133625 effective words/s\n",
      "2024-08-18 12:25:52,009 | INFO | word2vec.py:1608 | _log_progress | EPOCH 53 - PROGRESS: at 88.23% examples, 208032 words/s, in_qsize 4, out_qsize 0\n",
      "2024-08-18 12:25:52,038 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 53: training on 260267 raw words (250450 effective words) took 1.1s, 230745 effective words/s\n",
      "2024-08-18 12:25:53,114 | INFO | word2vec.py:1608 | _log_progress | EPOCH 54 - PROGRESS: at 80.71% examples, 193528 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:25:53,301 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 54: training on 260267 raw words (250456 effective words) took 1.2s, 202306 effective words/s\n",
      "2024-08-18 12:25:54,453 | INFO | word2vec.py:1608 | _log_progress | EPOCH 55 - PROGRESS: at 96.03% examples, 233638 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:25:54,519 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 55: training on 260267 raw words (250466 effective words) took 1.1s, 229632 effective words/s\n",
      "2024-08-18 12:25:55,579 | INFO | word2vec.py:1608 | _log_progress | EPOCH 56 - PROGRESS: at 96.20% examples, 236464 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:25:55,635 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 56: training on 260267 raw words (250381 effective words) took 1.1s, 233175 effective words/s\n",
      "2024-08-18 12:25:56,715 | INFO | word2vec.py:1608 | _log_progress | EPOCH 57 - PROGRESS: at 100.00% examples, 239086 words/s, in_qsize 0, out_qsize 1\n",
      "2024-08-18 12:25:56,715 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 57: training on 260267 raw words (250260 effective words) took 1.0s, 238624 effective words/s\n",
      "2024-08-18 12:25:57,826 | INFO | word2vec.py:1608 | _log_progress | EPOCH 58 - PROGRESS: at 65.68% examples, 155518 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:25:58,060 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 58: training on 260267 raw words (250313 effective words) took 1.3s, 195813 effective words/s\n",
      "2024-08-18 12:25:58,972 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 59: training on 260267 raw words (250538 effective words) took 0.8s, 323681 effective words/s\n",
      "2024-08-18 12:26:00,150 | INFO | word2vec.py:1608 | _log_progress | EPOCH 60 - PROGRESS: at 57.94% examples, 142414 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:26:00,804 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 60: training on 260267 raw words (250273 effective words) took 1.7s, 150652 effective words/s\n",
      "2024-08-18 12:26:01,808 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 61: training on 260267 raw words (250390 effective words) took 1.0s, 259736 effective words/s\n",
      "2024-08-18 12:26:02,881 | INFO | word2vec.py:1608 | _log_progress | EPOCH 62 - PROGRESS: at 53.97% examples, 126866 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:26:03,479 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 62: training on 260267 raw words (250451 effective words) took 1.7s, 150514 effective words/s\n",
      "2024-08-18 12:26:04,559 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 63: training on 260267 raw words (250366 effective words) took 1.0s, 259460 effective words/s\n",
      "2024-08-18 12:26:05,674 | INFO | word2vec.py:1608 | _log_progress | EPOCH 64 - PROGRESS: at 80.71% examples, 183871 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:26:05,916 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 64: training on 260267 raw words (250413 effective words) took 1.3s, 187066 effective words/s\n",
      "2024-08-18 12:26:06,918 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 65: training on 260267 raw words (250253 effective words) took 1.0s, 254361 effective words/s\n",
      "2024-08-18 12:26:07,859 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 66: training on 260267 raw words (250246 effective words) took 0.9s, 272426 effective words/s\n",
      "2024-08-18 12:26:08,952 | INFO | word2vec.py:1608 | _log_progress | EPOCH 67 - PROGRESS: at 100.00% examples, 237477 words/s, in_qsize 0, out_qsize 1\n",
      "2024-08-18 12:26:08,952 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 67: training on 260267 raw words (250296 effective words) took 1.1s, 237106 effective words/s\n",
      "2024-08-18 12:26:09,808 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 68: training on 260267 raw words (250410 effective words) took 0.8s, 301056 effective words/s\n",
      "2024-08-18 12:26:10,755 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 69: training on 260267 raw words (250240 effective words) took 0.9s, 268825 effective words/s\n",
      "2024-08-18 12:26:11,584 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 70: training on 260267 raw words (250246 effective words) took 0.8s, 314776 effective words/s\n",
      "2024-08-18 12:26:12,753 | INFO | word2vec.py:1608 | _log_progress | EPOCH 71 - PROGRESS: at 100.00% examples, 231580 words/s, in_qsize 0, out_qsize 1\n",
      "2024-08-18 12:26:12,755 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 71: training on 260267 raw words (250373 effective words) took 1.1s, 231229 effective words/s\n",
      "2024-08-18 12:26:13,669 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 72: training on 260267 raw words (250284 effective words) took 0.8s, 304775 effective words/s\n",
      "2024-08-18 12:26:14,720 | INFO | word2vec.py:1608 | _log_progress | EPOCH 73 - PROGRESS: at 100.00% examples, 247693 words/s, in_qsize 0, out_qsize 1\n",
      "2024-08-18 12:26:14,723 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 73: training on 260267 raw words (250440 effective words) took 1.0s, 246929 effective words/s\n",
      "2024-08-18 12:26:15,858 | INFO | word2vec.py:1608 | _log_progress | EPOCH 74 - PROGRESS: at 96.20% examples, 228443 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:26:15,881 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 74: training on 260267 raw words (250310 effective words) took 1.1s, 232337 effective words/s\n",
      "2024-08-18 12:26:17,046 | INFO | word2vec.py:1608 | _log_progress | EPOCH 75 - PROGRESS: at 80.73% examples, 184695 words/s, in_qsize 5, out_qsize 0\n",
      "2024-08-18 12:26:17,170 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 75: training on 260267 raw words (250425 effective words) took 1.2s, 205575 effective words/s\n",
      "2024-08-18 12:26:18,040 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 76: training on 260267 raw words (250420 effective words) took 0.8s, 298373 effective words/s\n",
      "2024-08-18 12:26:19,200 | INFO | word2vec.py:1608 | _log_progress | EPOCH 77 - PROGRESS: at 96.27% examples, 223168 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:26:19,231 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 77: training on 260267 raw words (250351 effective words) took 1.1s, 225653 effective words/s\n",
      "2024-08-18 12:26:20,028 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 78: training on 260267 raw words (250403 effective words) took 0.7s, 346328 effective words/s\n",
      "2024-08-18 12:26:21,100 | INFO | word2vec.py:1608 | _log_progress | EPOCH 79 - PROGRESS: at 92.07% examples, 221802 words/s, in_qsize 2, out_qsize 2\n",
      "2024-08-18 12:26:21,153 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 79: training on 260267 raw words (250412 effective words) took 1.1s, 229091 effective words/s\n",
      "2024-08-18 12:26:21,924 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 80: training on 260267 raw words (250417 effective words) took 0.7s, 360998 effective words/s\n",
      "2024-08-18 12:26:22,855 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 81: training on 260267 raw words (250403 effective words) took 0.9s, 283031 effective words/s\n",
      "2024-08-18 12:26:23,719 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 82: training on 260267 raw words (250490 effective words) took 0.8s, 327950 effective words/s\n",
      "2024-08-18 12:26:24,802 | INFO | word2vec.py:1608 | _log_progress | EPOCH 83 - PROGRESS: at 92.24% examples, 230668 words/s, in_qsize 2, out_qsize 1\n",
      "2024-08-18 12:26:24,840 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 83: training on 260267 raw words (250333 effective words) took 1.0s, 241444 effective words/s\n",
      "2024-08-18 12:26:25,620 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 84: training on 260267 raw words (250415 effective words) took 0.8s, 326545 effective words/s\n",
      "2024-08-18 12:26:26,673 | INFO | word2vec.py:1608 | _log_progress | EPOCH 85 - PROGRESS: at 80.52% examples, 194269 words/s, in_qsize 6, out_qsize 0\n",
      "2024-08-18 12:26:26,837 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 85: training on 260267 raw words (250447 effective words) took 1.2s, 208679 effective words/s\n",
      "2024-08-18 12:26:27,443 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 86: training on 260267 raw words (250380 effective words) took 0.6s, 430374 effective words/s\n",
      "2024-08-18 12:26:28,342 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 87: training on 260267 raw words (250355 effective words) took 0.8s, 300553 effective words/s\n",
      "2024-08-18 12:26:28,927 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 88: training on 260267 raw words (250438 effective words) took 0.6s, 441460 effective words/s\n",
      "2024-08-18 12:26:29,648 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 89: training on 260267 raw words (250256 effective words) took 0.7s, 364432 effective words/s\n",
      "2024-08-18 12:26:30,548 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 90: training on 260267 raw words (250349 effective words) took 0.9s, 284904 effective words/s\n",
      "2024-08-18 12:26:31,549 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 91: training on 260267 raw words (250349 effective words) took 1.0s, 259013 effective words/s\n",
      "2024-08-18 12:26:32,601 | INFO | word2vec.py:1608 | _log_progress | EPOCH 92 - PROGRESS: at 96.03% examples, 239643 words/s, in_qsize 1, out_qsize 1\n",
      "2024-08-18 12:26:32,617 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 92: training on 260267 raw words (250394 effective words) took 1.0s, 246088 effective words/s\n",
      "2024-08-18 12:26:33,650 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 93: training on 260267 raw words (250396 effective words) took 0.9s, 264933 effective words/s\n",
      "2024-08-18 12:26:34,750 | INFO | word2vec.py:1608 | _log_progress | EPOCH 94 - PROGRESS: at 100.00% examples, 242223 words/s, in_qsize 0, out_qsize 1\n",
      "2024-08-18 12:26:34,750 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 94: training on 260267 raw words (250456 effective words) took 1.0s, 242048 effective words/s\n",
      "2024-08-18 12:26:35,485 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 95: training on 260267 raw words (250356 effective words) took 0.7s, 350166 effective words/s\n",
      "2024-08-18 12:26:36,475 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 96: training on 260267 raw words (250382 effective words) took 0.9s, 271052 effective words/s\n",
      "2024-08-18 12:26:37,386 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 97: training on 260267 raw words (250408 effective words) took 0.8s, 303302 effective words/s\n",
      "2024-08-18 12:26:38,231 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 98: training on 260267 raw words (250427 effective words) took 0.8s, 328950 effective words/s\n",
      "2024-08-18 12:26:39,347 | INFO | word2vec.py:1652 | _log_epoch_end | EPOCH 99: training on 260267 raw words (250392 effective words) took 1.0s, 260358 effective words/s\n",
      "2024-08-18 12:26:39,349 | INFO | utils.py:447 | add_lifecycle_event | Word2Vec lifecycle event {'msg': 'training on 26026700 raw words (25037894 effective words) took 112.0s, 223629 effective words/s', 'datetime': '2024-08-18T12:26:39.349412', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-08-18 12:26:39,350 | INFO | utils.py:447 | add_lifecycle_event | Word2Vec lifecycle event {'params': 'Word2Vec<vocab=19345, vector_size=300, alpha=0.025>', 'datetime': '2024-08-18T12:26:39.350282', 'gensim': '4.3.3', 'python': '3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "WORD2VEC_DIM = 300\n",
    "SG = {\"CBOW\":0,\"SKIP_GRAM\":1}\n",
    "corpus = [advertools.word_tokenize(text,phrase_len=1)[0] for text in data_preprocess[\"tweet\"].values]\n",
    "word2vec = gensim.models.Word2Vec(corpus,window=5,vector_size=WORD2VEC_DIM,min_count=1,epochs=100,sg=SG[\"CBOW\"])\n",
    "WORD2VEC_VOCAB_SIZE = len(word2vec.wv.index_to_key)+1\n",
    "word2vec_embedding = np.zeros((WORD2VEC_VOCAB_SIZE,WORD2VEC_DIM))\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if word in word2vec.wv.index_to_key:\n",
    "        word2vec_embedding[index] = word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47ef69e2-e12d-452a-b63d-3f1646c04778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19346, 300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c7c2765-b624-4f99-a50a-56908d9bf6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "EMBEDDINGS_DIMENSION = 300\n",
    "glove_embeddings = np.zeros((GLOVE_VOCAB_SIZE,EMBEDDINGS_DIMENSION))\n",
    "glove_model = spacy.load(\"en_core_web_lg\")\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if word in glove_model.vocab.strings:\n",
    "        glove_embeddings[index] = glove_model.vocab[word].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24d283c5-7507-4215-a791-3bcefa6e95c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17559, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "986dae19-a0b7-462b-a443-8aef9b222dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5548330-08f4-41f4-aab2-965093380001",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.txt\",\"w\", encoding=\"utf-8\") as file:\n",
    "    for doc in data_preprocess[\"tweet\"].values:\n",
    "        file.write(doc + \"\\n\")\n",
    "MODEL_TYPE = \"skipgram\"\n",
    "EMBEDDINGS_DIMENSION = 300\n",
    "FASTTEXT_VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "fasttext_embeddings = np.zeros((FASTTEXT_VOCAB_SIZE,EMBEDDINGS_DIMENSION))\n",
    "fasttext_model = fasttext.train_unsupervised(\"corpus.txt\",ws=5,minn=2,epoch=100,dim=300,model=MODEL_TYPE)\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if word in fasttext_model.words:\n",
    "        fasttext_embeddings[index] = fasttext_model.get_word_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5144a074-e91c-44a5-8483-5c11aa0a2bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17559, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c46b4d1f-54eb-41ca-8b42-9b6654bd86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff3cb645-e7db-42dd-beb5-00975d6b1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELMO_VOCAB_SIZE = len(data_preprocess)\n",
    "EMBEDDINGS_DIMENSION = 1024\n",
    "\n",
    "elmo_embeddings = np.zeros((ELMO_VOCAB_SIZE,EMBEDDINGS_DIMENSION))\n",
    "elmo = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\")\n",
    "for index,text in enumerate(data_preprocess[\"tweet\"].values):\n",
    "    print(\"index\",index)\n",
    "    embeddings = elmo(tf.constant([text]))\n",
    "    elmo_embeddings[index]=embeddings.numpy()\n",
    "elmo_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19b598a0-1026-43a3-9c67-d7aaafd98904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee725cf6-dd40-40a2-8008-234fae5ba2b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BERT_VOCAB_SIZE = len(data_preprocess)\n",
    "EMBEDDINGS_DIMENSION = 768\n",
    "\n",
    "bert_embeddings = np.zeros((BERT_VOCAB_SIZE,EMBEDDINGS_DIMENSION))\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "for index,text in enumerate(data_preprocess[\"tweet\"].values):\n",
    "    print(\"index\",index)\n",
    "    inputs = bert_tokenizer(text, padding=True,return_tensors='tf')\n",
    "    outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    cls_embeddings = embeddings[:, 0, :]\n",
    "    bert_embeddings[index]=cls_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b58e38de-c210-4883-9a32-251a7ba41334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20758, 768)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e871b94-52b9-46db-9708-08a27426c513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_sequence = max([len(doc) for doc in corpus])\n",
    "max_length_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca9d3b05-9061-4142-9fae-cbc4b19d29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(data_preprocess[\"tweet\"].values)\n",
    "padded_sequence = pad_sequences(sequence,maxlen=max_length_sequence,padding=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21cf46de-62d9-481e-a920-59a5d36107c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  147,   77,    7],\n",
       "       [   0,    0,    0, ...,    2,   53,   58],\n",
       "       [   0,    0,    0, ...,   94,  179, 1138],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0, 1019,   25],\n",
       "       [   0,    0,    0, ...,  668,  271,   18],\n",
       "       [   0,    0,    0, ..., 2095, 1592, 2583]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6e2c2fa-489a-4eea-ad6f-14d6d69a4450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19346, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee481cb-e7fd-47d7-9eb4-f0fded633b3c",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a4e900d-757d-4fc1-a2bd-3a53746fc832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-18 12:29:36,672 | WARNING | lstm.py:590 | __init__ | Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "embedding = word2vec_embedding\n",
    "Embedding_layer = Embedding(WORD2VEC_VOCAB_SIZE,WORD2VEC_DIM,weights = [embedding],input_length = max_length_sequence, trainable=False)\n",
    "model = Sequential()\n",
    "model.add(Embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(1,\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ed838e6b-a74c-4b2d-a5c1-54327a9fa761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 186, 300)          5803800   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 186, 300)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               160400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,964,301\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 5,803,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7059cef6-deb7-4727-8e07-4597207be9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disorder</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>feelingtake good care growing heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>belly laughing face tear joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Disorder                                tweet\n",
       "0  SCHIZOPHRENIA  feelingtake good care growing heart\n",
       "1           ADHD         belly laughing face tear joy"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocess.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0ff9c9c-dc9f-41bc-9562-0c78b1e46faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Disorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feelingtake good care growing heart</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>belly laughing face tear joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relationship people concealed motif aware othe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>guna honest brain though spongebob saw creeper</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  Disorder\n",
       "0                 feelingtake good care growing heart         1\n",
       "1                        belly laughing face tear joy         1\n",
       "4   relationship people concealed motif aware othe...         1\n",
       "9      guna honest brain though spongebob saw creeper         1\n",
       "12                                             thanks         1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bin = data_preprocess[[\"tweet\",\"Disorder\"]]\n",
    "diagnosed_group = data_bin[data_bin[\"Disorder\"]!=\"CONTROL\"]\n",
    "diagnosed_group[\"Disorder\"] = \"DIAGNOSED\"\n",
    "control_group = data_bin[data_bin[\"Disorder\"]==\"CONTROL\"]\n",
    "data_bin = pd.concat([diagnosed_group,control_group],axis=0)\n",
    "encode_target = {\"DIAGNOSED\":1,\n",
    "                \"CONTROL\":0}\n",
    "data_bin[\"Disorder\"] = data_bin[\"Disorder\"].map(encode_target)\n",
    "data_bin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c77f750-e5fc-4c85-bd5b-9e1cc3213e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disorder\n",
       "0    18640\n",
       "1    18232\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bin['Disorder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f88792c4-be68-47dc-8f14-91ff612445fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_bin[\"Disorder\"].values\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(padded_sequence, y, test_size=0.3, random_state=42,stratify=y)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42,stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2541283b-4af2-40b2-babb-dbcc93ec4ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25810, 186), (8849, 186), (2213, 186))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_val.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27e512fc-7b15-455d-9129-dd5034ba71d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=int64), array([13048, 12762], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([4473, 4376], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([1119, 1094], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train,return_counts=True))\n",
    "print(np.unique(y_val,return_counts=True))\n",
    "print(np.unique(y_test,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df609fa0-7b60-4fcb-a05f-1117eea19689",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbff5f0d-f306-4655-86d6-9eae7f3b2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "446/468 [===========================>..] - ETA: 29s - loss: 0.2995 - accuracy: 0.8874"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=5,batch_size=32,validation_data=(X_val, y_val),verbose=1,callbacks=[early_stopping],\n",
    "                   workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17139696-9371-451f-a259-0208937474ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 9s 72ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2cf7cf8-cfbe-413f-a821-f64beb92e151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32711464, 0.9879332 , 0.9888914 , ..., 0.9932704 , 0.9756116 ,\n",
       "       0.99673086], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.squeeze(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "965907c1-e772-4348-8207-6c9b0ce01b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.where(y_pred<0.6,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ca031d6-279f-490d-ac8f-87bc5fbd6bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.527933</td>\n",
       "      <td>0.916711</td>\n",
       "      <td>0.374257</td>\n",
       "      <td>0.953661</td>\n",
       "      <td>0.438007</td>\n",
       "      <td>0.934821</td>\n",
       "      <td>0.883189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision_0  precision_1  recall_0  recall_1      f1_0      f1_1  accuracy\n",
       "0     0.527933     0.916711  0.374257  0.953661  0.438007  0.934821  0.883189"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance_lstm = classification_metrics(y_test,y_pred)\n",
    "model_performance_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ab9a0a5-cf88-4828-a76e-d1bb122dfe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 10s 77ms/step - loss: 0.3450 - accuracy: 0.8895\n"
     ]
    }
   ],
   "source": [
    "test_loss,accuracy = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb45391-c4e7-4c00-9b3e-36ad5dbaac7a",
   "metadata": {},
   "source": [
    "### CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2021f25-2484-4dae-a4c3-4b309e8dea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_layer = Embedding(WORD2VEC_VOCAB_SIZE,WORD2VEC_DIM,weights = [word2vec_embedding],input_length = max_length_sequence, trainable=False)\n",
    "model = Sequential()\n",
    "model.add(Embedding_layer)\n",
    "model.add(Conv1D(filters=32,kernel_size=5,activation=\"relu\",padding=\"same\",strides=1))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54eb31de-d091-4c97-a97a-c8d955d512b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 186, 300)          5268600   \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 186, 32)           48032     \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPoolin  (None, 93, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                3232      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5373097 (20.50 MB)\n",
      "Trainable params: 104497 (408.19 KB)\n",
      "Non-trainable params: 5268600 (20.10 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e23b8517-9b0a-48f7-93e2-9eaabd537e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "468/468 [==============================] - 90s 181ms/step - loss: 0.2979 - accuracy: 0.8891 - val_loss: 0.2727 - val_accuracy: 0.8928\n",
      "Epoch 2/5\n",
      "468/468 [==============================] - 84s 180ms/step - loss: 0.2443 - accuracy: 0.9068 - val_loss: 0.2774 - val_accuracy: 0.8838\n",
      "Epoch 3/5\n",
      "468/468 [==============================] - 96s 205ms/step - loss: 0.2099 - accuracy: 0.9194 - val_loss: 0.2939 - val_accuracy: 0.8844\n",
      "Epoch 4/5\n",
      "468/468 [==============================] - 84s 180ms/step - loss: 0.1736 - accuracy: 0.9297 - val_loss: 0.3250 - val_accuracy: 0.8844\n",
      "Epoch 4: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "history = model.fit(X_train,y_train,epochs=5,batch_size=32,validation_split=0.1,verbose=1,callbacks=[early_stopping],\n",
    "                   workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4968fbf2-f150-4049-a832-e999158c1699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 4s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.522026</td>\n",
       "      <td>0.927528</td>\n",
       "      <td>0.469307</td>\n",
       "      <td>0.940499</td>\n",
       "      <td>0.494265</td>\n",
       "      <td>0.933969</td>\n",
       "      <td>0.883189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision_0  precision_1  recall_0  recall_1      f1_0      f1_1  accuracy\n",
       "0     0.522026     0.927528  0.469307  0.940499  0.494265  0.933969  0.883189"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.squeeze(y_pred)\n",
    "y_pred = np.where(y_pred<0.6,0,1)\n",
    "model_performance = classification_metrics(y_test,y_pred)\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83f98068-dadb-4fed-b4af-d7e5d38a6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 4s 30ms/step - loss: 0.2976 - accuracy: 0.8907\n"
     ]
    }
   ],
   "source": [
    "test_loss,accuracy = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa0571-bc30-49b7-9448-10d2245571b9",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d3570412-aac2-4358-9770-64062ff97e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f32438e3-8c42-4b7b-8270-a57e8e8b27f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16606, 1), (4152, 1))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bert = data_bin.copy()\n",
    "data_bert[\"tweet\"] = \"[CLS] \" +data_bert['tweet'] + \"[SEP]\"\n",
    "X = data_bert[[\"tweet\"]]\n",
    "y = data_bert[[\"Disorder\"]]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
    "\n",
    "train_text = X_train[\"tweet\"].values.tolist()\n",
    "train_label = y_train[\"Disorder\"].values.tolist()\n",
    "\n",
    "test_text = X_test[\"tweet\"].values.tolist()\n",
    "test_label = y_test[\"Disorder\"].values.tolist()\n",
    "\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7debd15b-c36e-4cd1-bf45-b3d3db165c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16606, 16606)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text),len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7108b5c1-9bbb-4dcf-a40f-664bfb9549e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4152, 4152)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_text),len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ffbde75c-b113-4e3c-8fd7-2a8c87c10440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disorder\n",
       "1    14585\n",
       "0     2021\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.Disorder.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a0d04548-3be6-49c7-9a3e-5c7f5e7ff93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disorder\n",
       "1    3647\n",
       "0     505\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.Disorder.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936913b-4ba5-4704-b4cb-cc4b49b10347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f559befa-ec21-48e0-b371-ac7d89df6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_TOKENS_LENGTH = 512\n",
    "max_length_sequence = min(max([len(doc) for doc in corpus]),BERT_TOKENS_LENGTH)\n",
    "train_encoding = bert_tokenizer.batch_encode_plus(train_text,\n",
    "                                              padding=True,\n",
    "                                              truncation=True,\n",
    "                                              max_length = max_length_sequence,\n",
    "                                              return_tensors='tf')\n",
    "test_encoding = bert_tokenizer.batch_encode_plus(test_text,\n",
    "                                              padding=True, \n",
    "                                            truncation=True,\n",
    "                                              max_length = max_length_sequence,\n",
    "                                              return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bcbe0174-d345-40a8-8527-c1041ffcd18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4152, 186])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoding[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9c1bfae3-8ce3-4e1b-80de-b04c73bba442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] people telling e plaining behaviour based theory psychosis doe help undo thing e perienced wa real happened remember way happened im stuck sorry ive helped[SEP]'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ba537236-5cc1-40d9-ba75-6ddbc2b87d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoding = bert_tokenizer(train_text,padding=True)\n",
    "# test_encoding = bert_tokenizer(test_text,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4a4ee6a3-0425-489d-a270-067d68aede66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encoding),train_label))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encoding),test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1397fc57-b848-4a87-814f-1d7bff1edeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT_TOKENS_LENGTH = 512\n",
    "# max_length_sequence = min(max([len(doc) for doc in corpus]),BERT_TOKENS_LENGTH)\n",
    "# max_length_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba9877a4-a7ac-4a1c-8725-a7c1cdd3ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9de3d4da-9004-4340-9327-ec6d4ff84696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with an appropriate optimizer, loss function, and metrics\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "bert_model.compile(optimizer=\"adam\", loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "89c085ad-abe5-4c5e-b024-3837eda6ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_673 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d3baf4df-b065-4b09-bbdc-e7c80c4444b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 22:49:55,911 | WARNING | ag_logging.py:142 | warning | AutoGraph could not transform <function infer_framework at 0x000001E5187447C0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function infer_framework at 0x000001E5187447C0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 22:50:02,340 | WARNING | module_wrapper.py:149 | _tfmw_add_deprecation_warning | From C:\\Anaconda\\envs\\mental_disorder_dev\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2024-08-01 22:50:21,285 | WARNING | module_wrapper.py:149 | _tfmw_add_deprecation_warning | From C:\\Anaconda\\envs\\mental_disorder_dev\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "2024-08-01 22:50:37,873 | WARNING | module_wrapper.py:149 | _tfmw_add_deprecation_warning | From C:\\Anaconda\\envs\\mental_disorder_dev\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519/519 [==============================] - 34538s 66s/step - loss: 0.3867 - accuracy: 0.8751 - val_loss: 0.3755 - val_accuracy: 0.8779\n",
      "Epoch 2/3\n",
      "519/519 [==============================] - 12262s 24s/step - loss: 0.3932 - accuracy: 0.8782 - val_loss: 0.3705 - val_accuracy: 0.8770\n",
      "Epoch 3/3\n",
      "519/519 [==============================] - 12800s 25s/step - loss: 0.3906 - accuracy: 0.8772 - val_loss: 0.3754 - val_accuracy: 0.8775\n"
     ]
    }
   ],
   "source": [
    "history = bert_model.fit(\n",
    "    [train_encoding['input_ids'], train_encoding['token_type_ids'], train_encoding['attention_mask']],\n",
    "    tf.convert_to_tensor(train_label, dtype=tf.int32),\n",
    "    validation_data=(\n",
    "      [test_encoding['input_ids'], test_encoding['token_type_ids'], test_encoding['attention_mask']], tf.convert_to_tensor(test_label, dtype=tf.int32)),\n",
    "    batch_size=32,\n",
    "    epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e52fffc7-7386-4001-bf42-06523e197753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 1453s 11s/step - loss: 0.3754 - accuracy: 0.8775\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = bert_model.evaluate(\n",
    "    [test_encoding['input_ids'], test_encoding['token_type_ids'], test_encoding['attention_mask']],\n",
    "    tf.convert_to_tensor(test_label)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a823d842-b2f1-4174-bdf6-8ca3ae2d6987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3753882050514221, 0.8775259256362915)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "58678a5d-2471-4c5e-a48f-5563a0cdfef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 1419s 11s/step\n"
     ]
    }
   ],
   "source": [
    "pred = bert_model.predict(\n",
    "    [test_encoding['input_ids'], test_encoding['token_type_ids'], test_encoding['attention_mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ce713cd2-e130-4093-9eb1-f7daaa36ea2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       ...,\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f8f11db7-e4b2-4ac5-b55f-d7a6e32f6a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       ...,\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ],\n",
       "       [-0.72870165,  1.5744926 ]], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = pred.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e08845be-c4c5-4668-b15b-058ba25a8f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4152,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = tf.argmax(logits, axis=1)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b8aadb6d-94ff-4fb7-b87a-df7110ea479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = pred_labels.numpy()\n",
    " \n",
    "label = {\n",
    "    1: 'positive',\n",
    "    0: 'Negative'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bcec00f5-7867-4a6d-92a7-21b918528c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ccb38-c2f1-4624-975d-2d8d4546cf86",
   "metadata": {},
   "source": [
    "## GAN-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4737a2-81ab-4e0a-bffa-89eb8af14e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0cd9b-6514-4c67-bc39-2b7fe7741a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
