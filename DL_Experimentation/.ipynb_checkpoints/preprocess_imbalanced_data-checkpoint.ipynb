{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e59138-474e-4f14-be83-f3221e8abccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ali\n",
      "[nltk_data]     Haider\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ali\n",
      "[nltk_data]     Haider\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import advertools\n",
    "import re\n",
    "import spacy\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report,auc, \n",
    "                            roc_auc_score, precision_score,\n",
    "                            recall_score,f1_score, accuracy_score)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import nltk.util\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf36dd8e-97bd-4660-ac3d-417ceee0788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent/\"Data\"\n",
    "final_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2101000-ca88-4381-9ae7-cf1b48932f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 622/622 [00:11<00:00, 53.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 124/124 [00:02<00:00, 59.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 170/170 [00:03<00:00, 50.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 136/136 [00:02<00:00, 58.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 1703/1703 [00:31<00:00, 54.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 249/249 [00:04<00:00, 59.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 55.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:01<00:00, 48.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 127/127 [00:02<00:00, 61.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 63.54it/s]\n"
     ]
    }
   ],
   "source": [
    "for child in data_path.iterdir():\n",
    "    for child_ch in child.iterdir():\n",
    "        if child_ch.is_dir():\n",
    "            files = list(child_ch.glob(\"**/*.csv\"))\n",
    "            for fls in tqdm(range(len(files))):\n",
    "                df = pd.read_csv(files[fls])\n",
    "        else:\n",
    "            df = pd.read_csv(child_ch)\n",
    "        final_data = pd.concat([df,final_data],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051b9f3d-f009-40d5-96dd-f3a50338d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data = final_data[final_data[\"class\"]!=\"CONTROL\"]\n",
    "pipeline_data = final_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47773f72-19c5-4f4e-acd3-bda9cc1c7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str) -> str:\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)\n",
    "    emoji_token = advertools.extract_emoji(tokens[0])\n",
    "    emoji_token = emoji_token[\"emoji_text\"]\n",
    "    for tok in range(0,len(emoji_token)):\n",
    "        if emoji_token[tok]:\n",
    "            tokens[0][tok] = \" \".join(emoji_token[tok])\n",
    "    clean_text = \" \".join(tokens[0])\n",
    "    clean_pattern = r\"@\\w+|#\\w+|\\W+|x+|https\\.\\//www\\.(\\w+|\\W+)\\.com|http\\w*|www\\.(\\w+|\\W+)\\.com|user\\w*|\\d+\"\n",
    "    clean_text = re.sub(clean_pattern,\" \",clean_text).strip()\n",
    "    clean_text = re.sub(r\"\\s+\",\" \",clean_text).strip()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48f5bc5-be40-4347-b960-e558d592ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatize(text: str) -> str:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)\n",
    "    text = [lemma.lemmatize(tok) for tok in tokens[0]]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22451da-abe0-486d-943e-61d4e7e9b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text: str) -> str:\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)[0]\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    clean_word = [tok for tok in tokens if tok not in stop_words]\n",
    "    return \" \".join(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8341aaa-92d7-4a29-82ca-aceb348cfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_pipeline(data_preprocess: pd.DataFrame,\n",
    "                            filename: str,\n",
    "                            ) -> pd.DataFrame:\n",
    "    data_preprocess = data_preprocess[~(data_preprocess[\"tweet\"].isnull())]\n",
    "    data_preprocess = data_preprocess[[\"class\",\"tweet\"]]\n",
    "    data_preprocess.rename(columns={\"class\":\"Disorder\"}, inplace=True)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(clean_text)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(text_lemmatize)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(remove_stopword)\n",
    "    data_preprocess = data_preprocess[data_preprocess[\"tweet\"]!=\"\"]\n",
    "    print(\"saving preprocessed data\")\n",
    "    data_preprocess.to_csv(filename,index=False)\n",
    "    print(\"data saved\")\n",
    "    return data_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17793e44-fb37-4581-a33d-2d8f4adf82cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessed data\n",
      "data saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disorder</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>sally white heart white heart wont forget angel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>personally life split two everything psychosis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>envisage sufferance look surfacing ridiculous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>take responsibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>im sorry angry want slap psychologist wet mop ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Disorder                                              tweet\n",
       "0  SCHIZOPHRENIA    sally white heart white heart wont forget angel\n",
       "1  SCHIZOPHRENIA  personally life split two everything psychosis...\n",
       "2  SCHIZOPHRENIA  envisage sufferance look surfacing ridiculous ...\n",
       "3  SCHIZOPHRENIA                                take responsibility\n",
       "4  SCHIZOPHRENIA  im sorry angry want slap psychologist wet mop ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocess = data_preprocess_pipeline(pipeline_data,\n",
    "                                           filename=\"binary_imbalanced_data_preprocess.csv\",\n",
    "                                           )\n",
    "data_preprocess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baefa47b-bbf7-4ccf-9f2d-712df2105030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disorder\n",
       "ADHD               3018\n",
       "SCHIZOPHRENIA      2959\n",
       "OCD                2888\n",
       "ANXIETY            2719\n",
       "CONTROL            2526\n",
       "PTSD               2458\n",
       "DEPRESSION         2143\n",
       "AUTISM             1402\n",
       "EATING DISORDER     402\n",
       "BIPOLAR             243\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocess.Disorder.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5f7ab-c018-4126-9647-8d71a81b7300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609c58f-30c6-4c1e-9856-78c979eaf6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab4555-938d-4cca-a3f5-83d7169e510e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
