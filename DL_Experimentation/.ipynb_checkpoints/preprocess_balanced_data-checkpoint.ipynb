{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e59138-474e-4f14-be83-f3221e8abccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ali\n",
      "[nltk_data]     Haider\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ali\n",
      "[nltk_data]     Haider\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import advertools\n",
    "import re\n",
    "import spacy\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report,auc, \n",
    "                            roc_auc_score, precision_score,\n",
    "                            recall_score,f1_score, accuracy_score)\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import nltk.util\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf36dd8e-97bd-4660-ac3d-417ceee0788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = Path.cwd().parent/\"Data\"\n",
    "binary_balanced_data=\"control_data.csv\"\n",
    "multi_balanced_data = \"multi_class_data_balanced.csv\"\n",
    "final_data = pd.read_csv(multi_balanced_data)\n",
    "final_data.rename(columns={\"Disorder\":\"class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051b9f3d-f009-40d5-96dd-f3a50338d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = final_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47773f72-19c5-4f4e-acd3-bda9cc1c7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text:str) -> str:\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)\n",
    "    emoji_token = advertools.extract_emoji(tokens[0])\n",
    "    emoji_token = emoji_token[\"emoji_text\"]\n",
    "    for tok in range(0,len(emoji_token)):\n",
    "        if emoji_token[tok]:\n",
    "            tokens[0][tok] = \" \".join(emoji_token[tok])\n",
    "    clean_text = \" \".join(tokens[0])\n",
    "    clean_pattern = r\"@\\w+|#\\w+|\\W+|x+|https\\.\\//www\\.(\\w+|\\W+)\\.com|http\\w*|www\\.(\\w+|\\W+)\\.com|user\\w*|\\d+\"\n",
    "    clean_text = re.sub(clean_pattern,\" \",clean_text).strip()\n",
    "    clean_text = re.sub(r\"\\s+\",\" \",clean_text).strip()\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48f5bc5-be40-4347-b960-e558d592ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatize(text: str) -> str:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)\n",
    "    text = [lemma.lemmatize(tok) for tok in tokens[0]]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a22451da-abe0-486d-943e-61d4e7e9b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text: str) -> str:\n",
    "    tokens = advertools.word_tokenize(text,phrase_len=1)[0]\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    clean_word = [tok for tok in tokens if tok not in stop_words]\n",
    "    return \" \".join(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8341aaa-92d7-4a29-82ca-aceb348cfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_pipeline(data_preprocess: pd.DataFrame,\n",
    "                            filename: str,\n",
    "                            ) -> pd.DataFrame:\n",
    "    data_preprocess = data_preprocess[~(data_preprocess[\"tweet\"].isnull())]\n",
    "    data_preprocess = data_preprocess[[\"class\",\"tweet\"]]\n",
    "    data_preprocess.rename(columns={\"class\":\"Disorder\"}, inplace=True)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(clean_text)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(text_lemmatize)\n",
    "    data_preprocess[\"tweet\"] = data_preprocess[\"tweet\"].apply(remove_stopword)\n",
    "    data_preprocess = data_preprocess[data_preprocess[\"tweet\"]!=\"\"]\n",
    "    print(\"saving preprocessed data\")\n",
    "    data_preprocess.to_csv(filename,index=False)\n",
    "    print(\"data saved\")\n",
    "    return data_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17793e44-fb37-4581-a33d-2d8f4adf82cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessed data\n",
      "data saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disorder</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCHIZOPHRENIA</td>\n",
       "      <td>feelingtake good care growing heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADHD</td>\n",
       "      <td>belly laughing face tear joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>solid tactic simeone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONTROL</td>\n",
       "      <td>partey control middle unmatched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PTSD</td>\n",
       "      <td>relationship people concealed motif aware othe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Disorder                                              tweet\n",
       "0  SCHIZOPHRENIA                feelingtake good care growing heart\n",
       "1           ADHD                       belly laughing face tear joy\n",
       "2        CONTROL                               solid tactic simeone\n",
       "3        CONTROL                    partey control middle unmatched\n",
       "4           PTSD  relationship people concealed motif aware othe..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocess = data_preprocess_pipeline(pipeline_data,\n",
    "                                           filename=\"binary_balanced_data_preprocess.csv\",\n",
    "                                           )\n",
    "data_preprocess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefa47b-bbf7-4ccf-9f2d-712df2105030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5f7ab-c018-4126-9647-8d71a81b7300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609c58f-30c6-4c1e-9856-78c979eaf6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab4555-938d-4cca-a3f5-83d7169e510e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
