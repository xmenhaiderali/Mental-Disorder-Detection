{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db15a286-fa7d-45cc-a85a-f2f20d7127b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score,f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb36112-32d8-43c7-84ae-7e6a75fea533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "max_length_sequecnce = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7747df3-8eb8-4c65-a975-f389edfc0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd67817-e8e1-4009-bdee-9b7e96388371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disorder\n",
      "EATING DISORDER    5283\n",
      "BIPOLAR            5186\n",
      "SCHIZOPHRENIA      4959\n",
      "PTSD               4936\n",
      "AUTISM             4860\n",
      "OCD                4843\n",
      "ADHD               4806\n",
      "DEPRESSION         4805\n",
      "ANXIETY            4707\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_preprocess = pd.read_csv(\"multi_balanced_data_preprocess.csv\")\n",
    "data_bin = data_preprocess[[\"tweet\", \"Disorder\"]]\n",
    "data_bin = data_bin[data_bin[\"Disorder\"] != \"CONTROL\"]\n",
    "# diagnosed_group = data_bin[data_bin[\"Disorder\"] != \"CONTROL\"]\n",
    "# diagnosed_group[\"Disorder\"] = \"DIAGNOSED\"\n",
    "# control_group = data_bin[data_bin[\"Disorder\"] == \"CONTROL\"]\n",
    "# data_bin = pd.concat([diagnosed_group, control_group], axis=0)\n",
    "# encode_target = {\"DIAGNOSED\": 1, \"CONTROL\": 0}\n",
    "# data_bin[\"Disorder\"] = data_bin[\"Disorder\"].map(encode_target)\n",
    "print(data_bin[\"Disorder\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c49c4-f068-4cd7-946a-6408af0da26f",
   "metadata": {},
   "source": [
    "### GAN-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "989aa2be-c60d-4beb-9809-e4ab8b89f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Set the maximum sequence length\n",
    "max_seq_length = 128\n",
    "\n",
    "def create_generator(latent_dim, embedding_dim):\n",
    "    # Define the generator model\n",
    "    generator_input = Input(shape=(latent_dim,), dtype='float32')\n",
    "    x = Dense(256, activation='relu')(generator_input)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(embedding_dim, activation='linear')(x)\n",
    "    generator = Model(generator_input, x, name='generator')\n",
    "    return generator\n",
    "\n",
    "def create_discriminator(input_shape):\n",
    "    # Define the discriminator model\n",
    "    discriminator_input = Input(shape=input_shape, dtype='float32')\n",
    "    x = Dense(512, activation='relu')(discriminator_input)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    discriminator = Model(discriminator_input, x, name='discriminator')\n",
    "    return discriminator\n",
    "\n",
    "# Parameters\n",
    "latent_dim = 100\n",
    "embedding_dim = 768\n",
    "# learning_rate 0.001\n",
    "# Create generator and discriminator\n",
    "generator = create_generator(latent_dim, embedding_dim)\n",
    "discriminator = create_discriminator((embedding_dim,))\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create the GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "generated_embedding = generator(gan_input)\n",
    "gan_output = discriminator(generated_embedding)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Function to encode text using BERT\n",
    "def encode_texts(texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_seq_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='tf',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    input_ids = tf.concat(input_ids, axis=0)\n",
    "    attention_masks = tf.concat(attention_masks, axis=0)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(texts):\n",
    "    input_ids, attention_masks = encode_texts(texts)\n",
    "    outputs = bert_model(input_ids, attention_mask=attention_masks)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Training the GAN-BERT model\n",
    "def train_gan_bert(texts, labels, epochs=10, batch_size=32):\n",
    "    real_labels = np.ones((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, len(texts), batch_size)\n",
    "        real_texts = [texts[i] for i in idx]\n",
    "        real_embeddings = get_bert_embeddings(real_texts)\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_embeddings = generator.predict(noise)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_embeddings, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_embeddings, real_labels)\n",
    "        \n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        g_loss = gan.train_on_batch(noise, real_labels)  # Use real_labels here\n",
    "        \n",
    "        print(f\"{epoch+1}/{epochs} [D loss: {d_loss_real[0] + d_loss_fake[0]}] [G loss: {g_loss}]\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9b0fd3-4f81-4b8c-9e49-02ade01580b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(actuals: np.array, preds: np.array) -> pd.DataFrame:\n",
    "    metrics_performance = pd.DataFrame()\n",
    "    precision_0 = precision_score(actuals,preds, pos_label=0)\n",
    "    precision_1 = precision_score(actuals,preds, pos_label=1)\n",
    "    recall_0 = recall_score(actuals,preds,pos_label=0)\n",
    "    recall_1 = recall_score(actuals,preds,pos_label=1)\n",
    "    f1_0 = f1_score(actuals,preds,pos_label=0)\n",
    "    f1_1 = f1_score(actuals,preds,pos_label=1)\n",
    "    accuracy = accuracy_score(actuals,preds)\n",
    "    metrics_performance[\"precision_0\"] = [precision_0]\n",
    "    metrics_performance[\"precision_1\"] = [precision_1]\n",
    "    metrics_performance[\"recall_0\"] = [recall_0]\n",
    "    metrics_performance[\"recall_1\"] = [recall_1]\n",
    "    metrics_performance[\"f1_0\"] = [f1_0]\n",
    "    metrics_performance[\"f1_1\"] = [f1_1]\n",
    "    metrics_performance[\"accuracy\"] = [accuracy]\n",
    "    return metrics_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b969d46f-3dab-4249-b184-14fa246c1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(actuals: np.array, preds: np.array, average:\"binary\") -> pd.DataFrame:\n",
    "    metrics_performance = pd.DataFrame()\n",
    "    precision = precision_score(actuals,preds,average=average)\n",
    "    recall = recall_score(actuals,preds,average=average)\n",
    "    f1 = f1_score(actuals,preds,average=average)\n",
    "    accuracy = accuracy_score(actuals,preds)\n",
    "    metrics_performance[\"precision\"] = [precision]\n",
    "    metrics_performance[\"recall\"] = [recall]\n",
    "    metrics_performance[\"f1\"] = [f1]\n",
    "    metrics_performance[\"accuracy\"] = [accuracy]\n",
    "    return metrics_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0ed658-7cdb-4013-a37d-548c777318ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_bin[[\"tweet\"]]\n",
    "y = data_bin[[\"Disorder\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60915404-c8d7-42ed-af7a-97d98bc95c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df91f62-fbbb-4a0b-a959-e336d0f49e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Anaconda\\envs\\mental_disorder_cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step\n",
      "1/10 [D loss: 1.1940953731536865] [G loss: 0.220872700214386]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/10 [D loss: 0.039488205686211586] [G loss: 0.006460283882915974]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "3/10 [D loss: 0.001762085739756003] [G loss: 0.00010567634308245033]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "4/10 [D loss: 0.0003586734101190814] [G loss: 4.347660706116585e-06]\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "5/10 [D loss: 3.56482897245769e-05] [G loss: 1.7233823257356562e-08]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "6/10 [D loss: 7.173366027735106e-06] [G loss: 2.089856065978779e-09]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "7/10 [D loss: 4.457669702591371e-06] [G loss: 4.467061387458671e-09]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "8/10 [D loss: 7.710558898045946e-07] [G loss: 4.4053275567376704e-13]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "9/10 [D loss: 3.7223866686690845e-07] [G loss: 6.021012410439147e-14]\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "10/10 [D loss: 4.90008976208974e-07] [G loss: 3.809942285349022e-17]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "texts = X_train[\"tweet\"].values.tolist()\n",
    "labels = y_train[\"Disorder\"].values.tolist()\n",
    "\n",
    "train_gan_bert(texts, labels, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec70827-8e17-4b8c-9f78-289663ff3344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399.46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[\"tweet\"].values)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c599adaf-ef1f-48ee-b69c-9ec12b52407d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4439"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[\"tweet\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88bb03ac-89db-4c7f-a426-39074659cb72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    }
   ],
   "source": [
    "train_preds = []\n",
    "# start_point = 0\n",
    "# end_point = 100\n",
    "# for batch in tqdm(range(len(X_train[\"tweet\"].values)//100)):\n",
    "for train_text in X_train[\"tweet\"].values.tolist()[:100]:\n",
    "    train_embeddings = get_bert_embeddings(train_text)\n",
    "    train_preds.append(np.ravel(discriminator.predict(train_embeddings[0:1]))[0])\n",
    "\n",
    "    # print(np.squeeze(discriminator.predict(test_embeddings[0:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23003cb9-fbad-4965-a412-349b0bd3ef10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for test_text in X_test[\"tweet\"].values[:100]:\n",
    "    test_embeddings = get_bert_embeddings(test_text)\n",
    "    test_preds.append(np.ravel(discriminator.predict(test_embeddings[0:1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69104622-1071-4cff-ba4d-0141a4cff197",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.array(train_preds)\n",
    "test_preds = np.array(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea984fdd-e027-43f6-8786-7df6a08b67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = np.where(train_pred<0.6,0,1)\n",
    "train_model_performance_lstm = classification_metrics(y_train,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f71f3f2-d737-4429-b58f-492ce0479c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_performance_table = classification_metrics(y_train[:100],train_preds)\n",
    "test_performance_table = classification_metrics(y_test[:100],test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94bdb1-8354-41a3-9646-72a3260e2062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
